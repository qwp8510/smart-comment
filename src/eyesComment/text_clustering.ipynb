{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport re\nfrom os import path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.utils import shuffle\nfrom tensorflow import keras\nimport sys\n\nsys.path.extend(['../input/smartevent/'])\nfrom tokenization import FullTokenizer\nimport random\nimport torch\nimport torch.nn as nn","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_torch(seed=1029):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertTokenizer():\n    def __init__(self, tokenizer, max_length=30, labels=None):\n        self.tokenizer = tokenizer\n        self.maxLength = max_length\n        self.labels = labels\n\n    def clean_whitespace(self, token):\n        _RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n        return _RE_COMBINE_WHITESPACE.sub(\" \", token).strip()\n\n    def to_lowercase(self, text):\n        # when tokenize chinese with english text, you must transform to lowercase for tokenize\n        return text.lower()\n\n    def get_id(self, token):\n        PAD = 0\n        if len(token) >= self.maxLength:\n            return self.tokenizer.convert_tokens_to_ids(token)[:self.maxLength]\n        else:\n            return self.tokenizer.convert_tokens_to_ids(token) + [PAD] * (self.maxLength - len(token))\n\n#     def fit(self, texts):\n#         for idx, text in enumerate(texts):\n#             if isinstance(text, str):\n#                 text = self.to_lowercase(text)\n#                 print(idx)\n#                 clean_text = self.clean_whitespace(text)\n#                 word_token = self.tokenizer.tokenize_chinese(clean_text)\n#                 yield np.array(self.get_id(word_token))\n\n    def fit(self, text):\n        text = '[CLS]' + self.to_lowercase(text)\n        clean_text = self.clean_whitespace(text)\n        word_token = self.tokenizer.tokenize_chinese(clean_text)\n        return np.array(self.get_id(word_token))","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CommentsDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        df,\n        feature_col,\n        label_col,\n        tokenizer,\n        device=None\n    ):\n        self.df = df\n        self.feature_col = feature_col\n        self.label_col = label_col\n        self.tokenizer = tokenizer\n        self.device = device\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, idx):\n        idx_row = self.df.iloc[idx]\n        feature = torch.tensor(self.tokenizer.fit(idx_row[self.feature_col]), dtype=torch.long).to(self.device)\n        label = torch.tensor(self.tokenizer.fit(idx_row[self.label_col]), dtype=torch.long).to(self.device)\n        return feature, label","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VOCAB_DIR = '/kaggle/input/smartevent/vocab.txt'\n\ndef convert_data(df):\n    head_cols = df.columns.tolist()[:7]\n    for _, row in df.iterrows():\n        h_data = [row[col] for col in head_cols]\n        yield h_data\n\ndef _load_dataframe(file_dir):\n    files = [path.join(file_dir, f) for f in os.listdir(file_dir) if f.endswith('.csv')]\n    df = pd.concat([pd.read_csv(open(f,'rU'), encoding='utf-8', engine='c') for f in files], ignore_index=True)\n    shuffled_df = shuffle(df).reset_index(drop=True)\n    return shuffled_df\n\ndef load_comments():\n    train_dir = '/kaggle/input/smartevent'\n    df = _load_dataframe(train_dir)\n    df['text'] = df['text'].astype('str')\n    print('text length: {}'.format(len(df['text'])))\n    split_length = int(np.ceil(len(df) * 0.8))\n    h_data = list(convert_data(df))\n    h_train = h_data[:split_length]\n    h_val = h_data[split_length:]\n    bertTokenizer = BertTokenizer(tokenizer=FullTokenizer(VOCAB_DIR), max_length=50)\n    train_dataset = CommentsDataset(df[:split_length], 'text', 'text', bertTokenizer, device)\n    val_dataset = CommentsDataset(df[split_length:], 'text', 'text', bertTokenizer, device)\n    return h_data, h_val, train_dataset, val_dataset","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.hid_dim = hid_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, src):\n        #src = [batch size, src len]\n        embedded = self.dropout(self.embedding(src))\n        #embedded = [batch size, src len, embdim]\n        outputs, (hidden, cell) = self.lstm(embedded)\n        #outputs = [batch size, src len, hid dim * n directions]\n        #hidden = [n layers * n directions, batch size, hid dim]\n        #cell = [n layers * n directions, batch size, hid dim]\n\n        #outputs are always from the top hidden layer\n        return outputs, hidden, cell\n    \n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.output_dim = output_dim\n        self.hid_dim = hid_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n        self.fc_out = nn.Linear(hid_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, inp, hidden, cell):\n        #inp = [batch size]\n        #hidden = [n layers * n directions, batch size, hid dim]\n        #cell = [n layers * n directions, batch size, hid dim]\n        inp = inp.unsqueeze(0)\n        #inp = [1, batch size]\n        embedded = self.dropout(self.embedding(inp))\n        #embedded = [1, batch size, emb dim]\n        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n        #output = [seq len, batch size, hid dim * n directions]\n        #hidden = [n layers * n directions, batch size, hid dim]\n        #cell = [n layers * n directions, batch size, hid dim]\n        \n        prediction = self.fc_out(output.squeeze(0))\n        #prediction = [batch size, output dim]\n        return prediction, hidden, cell\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder=None, device=None):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        \n        assert encoder.hid_dim == decoder.hid_dim, \\\n            \"Hidden dimensions of encoder and decoder must be equal!\"\n        assert encoder.n_layers == decoder.n_layers, \\\n            \"Encoder and decoder must have equal number of layers!\"\n        \n    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n        #teacher_forcing_ratio is probability to use teacher forcing\n        trg_vocab_size = self.decoder.output_dim\n        #tensor to store decoder outputs\n        target_length = trg.size(1)\n        batch_size = trg.size(0)\n        outputs = torch.zeros(target_length, batch_size, trg_vocab_size).to(self.device)\n\n        #last hidden state of the encoder is used as the initial hidden state of the decoder\n        result, hidden, cell = self.encoder(src)\n        if not self.decoder:\n            return result\n        #first input to the decoder is the [CLS] tokens\n        inp = trg[:, 0]\n        \n        for t in range(target_length):\n            output, hidden, cell = self.decoder(inp, hidden, cell)\n            outputs[t] = output\n            #decide if we are going to use teacher forcing or not\n            teacher_force = random.random() < teacher_forcing_ratio\n            \n            #get the highest predicted token from our predictions\n            top1 = output.argmax(1)\n            inp = trg[:, t] if teacher_force else top1\n\n        return outputs","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Trainer():\n    def __init__(self, batch_size, model, optimizer, epochs, device):\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.model = model.to(device)\n        self.optimizer = optimizer\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def _save_model(self, epoch):\n        model_path = str(epoch)\n        torch.save(self.model.state_dict(), model_path)\n\n    def train_epoch(self, train_loader):\n        self.model.train()\n        avg_loss = 0.\n        for x_batch, y_batch in train_loader:\n            y_pred = self.model(x_batch, y_batch)\n            pred_dim = y_pred.shape[-1]\n            output = y_pred.view(-1, pred_dim)\n            y_batch = y_batch.view(-1)\n            loss = self.loss_fn(output, y_batch)\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            avg_loss += loss.item() / len(train_loader)\n        return avg_loss\n\n    def train_val(self, val_loader):\n        avg_loss = 0.\n        for x_batch, y_batch in val_loader:\n            y_pred = self.model(x_batch, y_batch).detach()\n            pred_dim = y_pred.shape[-1]\n            output = y_pred.view(-1, pred_dim)\n            y_batch = y_batch.view(-1)\n            avg_loss += self.loss_fn(output, y_batch).item() / len(val_loader)\n        return avg_loss\n\n    def fit(self, train_dataset, valid_dataset):\n        train_loader = torch.utils.data.DataLoader(\n            train_dataset, batch_size=self.batch_size, shuffle=True)\n        valid_loader = torch.utils.data.DataLoader(\n            valid_dataset, batch_size=self.batch_size, shuffle=False)\n        for epoch in range(self.epochs):\n            print('training on epoch: {}'.format(epoch))\n            avg_train_loss = self.train_epoch(train_loader)\n            avg_val_loss = self.train_val(valid_loader)\n            print('Epoch {} train loss: {}, val loss: {}'.format(\n                epoch, avg_train_loss, avg_val_loss))\n            self._save_model(epoch)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def main():\n    max_features = 21128\n    BATCH_SIZE = 128\n    INPUT_DIM = max_features\n    OUTPUT_DIM = max_features\n    ENC_EMB_DIM = 256\n    DEC_EMB_DIM = 256\n    HID_DIM = 512\n    N_LAYERS = 2\n    ENC_DROPOUT = 0.5\n    DEC_DROPOUT = 0.5\n    h_train, h_val, train_dataset, valid_dataset = load_comments()\n#     h_train, h_val, t_train, t_val = train_test_split(h_train, text_train, test_size=0.2, random_state=42)\n    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n    model = Seq2Seq(enc, dec, device)\n    optimizer = torch.optim.Adam(model.parameters())\n\n    trainer = Trainer(\n        batch_size=BATCH_SIZE,\n        model=model,\n        optimizer=optimizer,\n        epochs=30,\n        device=device)\n    trainer.fit(train_dataset, valid_dataset)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_torch(1029)\nmain()","execution_count":10,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: 'U' mode is deprecated\n  # This is added back by InteractiveShellApp.init_path()\n","name":"stderr"},{"output_type":"stream","text":"text length: 1655429\ntraining on epoch: 0\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-312b5f9bbdc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mseed_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1029\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-4749d004bc39>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         device=device)\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-cd81eb1265f5>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_dataset, valid_dataset)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training on epoch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mavg_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             print('Epoch {} train loss: {}, val loss: {}'.format(\n","\u001b[0;32m<ipython-input-7-cd81eb1265f5>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}